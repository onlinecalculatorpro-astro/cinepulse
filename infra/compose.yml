# CinePulse pipeline in runtime order:
#
# 1. scheduler  + webhooks  -> put "fetch/ingest this source" jobs into Redis (queue: events)
# 2. workers                 -> consume "events", normalize each story, enqueue to "sanitize"
# 3. sanitizer               -> consume "sanitize", dedupe + publish to Redis feed, fanout, push
# 4. api                     -> serve final deduped feed to app / cron push
#
# redis backs all queues and also stores feed data

x-env: &defaults
  ENV: prod
  TZ: Asia/Kolkata

  # Shared Redis (also used as RQ broker)
  REDIS_URL: redis://redis:6379/0

  # Public feed list key (Redis LIST). The sanitizer writes into this.
  FEED_KEY: feed:items

  # Source config for scheduler / webhooks to know what to poll
  SOURCES_FILE: /app/infra/source.yml

  # External API creds that some jobs may use
  TMDB_API_KEY: ${TMDB_API_KEY:-}

services:
  # --------------------------------------------------
  # Redis: single backing store for queues + feed data
  # --------------------------------------------------
  redis:
    image: redis:7-alpine
    command:
      - redis-server
      - --save
      - ""
      - --appendonly
      - "yes"
    restart: unless-stopped
    volumes:
      - redis-data:/data
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: sh -c 'redis-cli ping | grep -q PONG'
      interval: 10s
      timeout: 3s
      retries: 5

  # --------------------------------------------------
  # SCHEDULER:
  # periodically enqueues poll jobs into "events"
  # (example: poll RSS / poll YouTube / etc.)
  # --------------------------------------------------
  scheduler:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: scheduler
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    command:
      - python
      - -m
      - apps.scheduler.main
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WEBHOOKS:
  # on-demand triggers (HTTP in) that can enqueue work
  # into the same "events" queue (manual refresh, etc.)
  # --------------------------------------------------
  webhooks:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: webhooks
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "127.0.0.1:18001:8000"
    command: uvicorn apps.webhooks.main:app --host 0.0.0.0 --port 8000
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WORKERS:
  # consume "events" queue
  # - pull external source (RSS / YouTube / etc.)
  # - normalize each story (title, summary, image, timestamps...)
  # - enqueue sanitized-ready story into "sanitize" queue
  #
  # IMPORTANT:
  # workers DO NOT write to the public feed list anymore.
  # they just forward normalized stories for sanitizer to judge.
  # --------------------------------------------------
  workers:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
      # rq also looks at RQ_REDIS_URL; set explicitly (don't rely on ${...} expansion)
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" events'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # SANITIZER:
  # consume "sanitize" queue
  # - dedupe using title+summary signature
  # - first one wins, later dupes dropped
  # - if ACCEPTED:
  #     * LPUSH into FEED_KEY (Redis LIST)
  #     * LTRIM to MAX_FEED_LEN
  #     * publish realtime fanout (pub/sub + stream)
  #     * optionally enqueue push notification job
  #
  # sanitizer is the ONLY writer to FEED_KEY.
  # --------------------------------------------------
  sanitizer:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults

      # Signature memory: Redis SET storing "we already covered this event"
      SEEN_KEY: feed:seen_signatures

      # Hard cap for FEED_KEY list length
      MAX_FEED_LEN: "200"

      # Realtime fanout targets for accepted stories
      FEED_PUBSUB: feed:pub
      FEED_STREAM: feed:stream
      FEED_STREAM_MAXLEN: "5000"

      # Whether sanitizer should enqueue push jobs for accepted stories
      ENABLE_PUSH_NOTIFICATIONS: "0"

      # rq also looks at RQ_REDIS_URL; set explicitly
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" sanitize'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # API:
  # read-only public surface
  # - reads FEED_KEY from Redis (which sanitizer maintains)
  # - exposed to the app and also used by your cron push script
  # --------------------------------------------------
  api:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: api
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
      sanitizer:
        condition: service_started
    ports:
      - "127.0.0.1:18000:8000"
    command: >
      gunicorn -w 2 -k uvicorn.workers.UvicornWorker
      -b 0.0.0.0:8000 apps.api.app.main:app
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 20s

volumes:
  redis-data: {}
