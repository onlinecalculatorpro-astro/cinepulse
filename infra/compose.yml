# CinePulse ingestion pipeline (runtime order)
#
#   1. scheduler + webhooks
#        - decide WHICH sources to poll (YouTube channels, RSS feeds, etc.)
#        - enqueue poll / ingest work into Redis queue "events"
#
#   2. workers
#        - rq worker listening on "events"
#        - fetch/parse each source item
#        - normalize it into a canonical story object
#        - enqueue that story into Redis queue "sanitize"
#
#   3. sanitizer
#        - rq worker listening on "sanitize"
#        - dedupe by meaning (title+summary signature)
#        - first story that covers an event wins; later dupes are dropped
#        - if ACCEPTED:
#             * LPUSH story JSON into FEED_KEY (Redis LIST)
#             * LTRIM that list to MAX_FEED_LEN
#             * publish realtime fanout (pub/sub + stream)
#             * optionally queue push notifications
#        - sanitizer is the ONLY writer that mutates the public feed list
#
#   4. api
#        - read-only service
#        - serves the final deduped feed from Redis to the client / cron push
#
# Redis is shared by all services:
#   - acts as RQ broker ("events", "sanitize", etc.)
#   - stores the feed list and dedupe signature set
#
# We persist Redis data in a named volume (redis-data:) so restarts don't wipe
# feed history or seen-signature memory.

x-env: &defaults
  ENV: prod
  TZ: Asia/Kolkata

  # Shared Redis URL (everything uses this)
  REDIS_URL: redis://redis:6379/0

  # Public feed list key in Redis. ONLY sanitizer writes here.
  FEED_KEY: feed:items

  # YAML source config used by scheduler / webhooks
  SOURCES_FILE: /app/infra/source.yml

  # External API keys if needed by jobs (safe to leave blank in dev)
  TMDB_API_KEY: ${TMDB_API_KEY:-}

services:
  # --------------------------------------------------
  # REDIS
  # --------------------------------------------------
  redis:
    image: redis:7-alpine
    command:
      - redis-server
      - --save
      - ""
      - --appendonly
      - "yes"
    restart: unless-stopped
    volumes:
      - redis-data:/data
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: sh -c 'redis-cli ping | grep -q PONG'
      interval: 10s
      timeout: 3s
      retries: 5

  # --------------------------------------------------
  # SCHEDULER
  # --------------------------------------------------
  # Periodically schedules ingestion work:
  #   - figures out which feeds/channels to poll
  #   - enqueues poll jobs into the "events" queue
  scheduler:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: scheduler
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    command:
      - python
      - -m
      - apps.scheduler.main
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WEBHOOKS
  # --------------------------------------------------
  # HTTP entrypoint for manual / on-demand ingestion triggers.
  # It can enqueue new work into "events" immediately.
  webhooks:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: webhooks
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "127.0.0.1:18001:8000"
    command: uvicorn apps.webhooks.main:app --host 0.0.0.0 --port 8000
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WORKERS
  # --------------------------------------------------
  # rq worker for the "events" queue:
  #   - fetches raw items (RSS entry, YT upload, etc.)
  #   - cleans + normalizes into our canonical "story" dict
  #   - enqueues that story into the "sanitize" queue
  #
  # workers do NOT touch FEED_KEY directly.
  workers:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
      # rq also reads RQ_REDIS_URL explicitly
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" events'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # SANITIZER
  # --------------------------------------------------
  # rq worker for the "sanitize" queue:
  #   - dedupes stories by meaning (title+summary signature)
  #   - first version of an event wins
  #   - if accepted:
  #       * LPUSH into FEED_KEY
  #       * LTRIM to MAX_FEED_LEN
  #       * publish realtime fanout
  #       * optional push notifications
  #
  # ONLY sanitizer is allowed to mutate FEED_KEY.
  sanitizer:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults

      # Redis SET of seen signatures (prevents dup stories)
      SEEN_KEY: feed:seen_signatures

      # Max feed list length that sanitizer enforces
      MAX_FEED_LEN: "200"

      # Realtime pub/sub + stream fanout targets
      FEED_PUBSUB: feed:pub
      FEED_STREAM: feed:stream
      FEED_STREAM_MAXLEN: "5000"

      # Whether sanitizer should enqueue push jobs for accepted stories
      ENABLE_PUSH_NOTIFICATIONS: "0"

      # rq connection var
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" sanitize'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # API
  # --------------------------------------------------
  # Purely read-only.
  # Serves the final, deduped feed from Redis (which sanitizer maintains).
  api:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: api
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
      sanitizer:
        condition: service_started
    ports:
      - "127.0.0.1:18000:8000"
    command: >
      gunicorn -w 2 -k uvicorn.workers.UvicornWorker
      -b 0.0.0.0:8000 apps.api.app.main:app
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 20s

volumes:
  redis-data: {}
