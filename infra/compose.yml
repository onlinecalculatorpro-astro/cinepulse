# CinePulse ingestion pipeline (runtime order)
#
#   1. scheduler + webhooks
#        - decide WHICH sources to poll (YouTube channels, RSS feeds, etc.)
#        - enqueue poll / ingest work into Redis queue "events"
#
#   2. workers
#        - rq worker listening on "events"
#        - fetch/parse each source item
#        - normalize it into a canonical story object
#        - enqueue that story into Redis queue "sanitize"
#
#   3. sanitizer
#        - rq worker listening on "sanitize"
#        - dedupe by meaning (title+summary signature)
#        - first story that covers an event wins; later dupes are dropped
#        - if ACCEPTED:
#             * LPUSH story JSON into FEED_KEY (Redis LIST)
#             * LTRIM that list to MAX_FEED_LEN
#             * publish realtime fanout (pub/sub + stream)
#             * optionally queue push notifications
#        - sanitizer is the ONLY writer that mutates the public feed list
#
#   4. api
#        - read-only service
#        - serves the final deduped feed from Redis to the client / cron push
#
# Redis is shared by all services:
#   - acts as RQ broker ("events", "sanitize", etc.)
#   - stores the feed list and dedupe signature set
#
# NOTE:
#   - We DO NOT do any DB writes elsewhere.
#   - We keep Redis data in a named volume (redis-data:) so container restarts
#     don't wipe feed history or dedupe memory.


x-env: &defaults
  ENV: prod
  TZ: Asia/Kolkata

  # Shared Redis URL (used by everything: scheduler, workers, sanitizer, api)
  REDIS_URL: redis://redis:6379/0

  # Public feed list key in Redis. Only sanitizer writes here.
  FEED_KEY: feed:items

  # Path to YAML source config consumed by scheduler/webhooks
  SOURCES_FILE: /app/infra/source.yml

  # External API keys for certain jobs (optional)
  TMDB_API_KEY: ${TMDB_API_KEY:-}


services:
  # --------------------------------------------------
  # REDIS
  # --------------------------------------------------
  redis:
    image: redis:7-alpine
    command:
      - redis-server
      - --save
      - ""
      - --appendonly
      - "yes"
    restart: unless-stopped
    volumes:
      - redis-data:/data
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: sh -c 'redis-cli ping | grep -q PONG'
      interval: 10s
      timeout: 3s
      retries: 5

  # --------------------------------------------------
  # SCHEDULER
  # --------------------------------------------------
  # Periodically decides "go poll X now" and enqueues work into the
  # "events" queue. It uses SOURCES_FILE (or env fallbacks) for which
  # RSS feeds / YouTube channels to poll, plus throttling rules.
  scheduler:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: scheduler
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    command:
      - python
      - -m
      - apps.scheduler.main
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WEBHOOKS
  # --------------------------------------------------
  # On-demand triggers (HTTP -> enqueue work into "events").
  # Useful for manual refresh, ad-hoc ingestion, etc.
  webhooks:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: webhooks
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "127.0.0.1:18001:8000"
    command: uvicorn apps.webhooks.main:app --host 0.0.0.0 --port 8000
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # WORKERS
  # --------------------------------------------------
  # rq worker that consumes the "events" queue:
  #   - fetches each source item (RSS entry, YT video, etc.)
  #   - normalizes it into our standard story dict
  #   - enqueues that story to the "sanitize" queue
  #
  # CRITICAL:
  # workers DO NOT mutate the public feed list. They never push
  # into FEED_KEY.
  workers:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
      # rq also checks RQ_REDIS_URL explicitly
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" events'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # SANITIZER
  # --------------------------------------------------
  # rq worker that consumes the "sanitize" queue:
  #   - builds canonical signature (title+summary)
  #   - drops duplicates (same event already covered)
  #   - if first time:
  #       * LPUSH story JSON into FEED_KEY
  #       * LTRIM feed list to MAX_FEED_LEN
  #       * publish realtime fanout (pub/sub + stream)
  #       * optionally enqueue push notif
  #
  # This is the ONLY service that is allowed to write/trim FEED_KEY.
  sanitizer:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: worker
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults

      # Dedup memory: Redis SET of "we already covered this event"
      SEEN_KEY: feed:seen_signatures

      # Feed list size ceiling (sanitizer enforces via LTRIM)
      MAX_FEED_LEN: "200"

      # Realtime fanout config (pub/sub + Redis stream)
      FEED_PUBSUB: feed:pub
      FEED_STREAM: feed:stream
      FEED_STREAM_MAXLEN: "5000"

      # Push notifications toggle
      ENABLE_PUSH_NOTIFICATIONS: "0"

      # rq connection var
      RQ_REDIS_URL: redis://redis:6379/0
    depends_on:
      redis:
        condition: service_healthy
    command: >
      sh -lc 'rq worker -u "${REDIS_URL:-redis://redis:6379/0}" sanitize'
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 10s

  # --------------------------------------------------
  # API
  # --------------------------------------------------
  # Read-only service that exposes the final feed as HTTP.
  # It reads FEED_KEY straight out of Redis (maintained by sanitizer).
  api:
    build:
      context: ..
      dockerfile: infra/Dockerfile
      target: api
    restart: unless-stopped
    env_file: ../.env
    environment:
      <<: *defaults
    depends_on:
      redis:
        condition: service_healthy
      sanitizer:
        condition: service_started
    ports:
      - "127.0.0.1:18000:8000"
    command: >
      gunicorn -w 2 -k uvicorn.workers.UvicornWorker
      -b 0.0.0.0:8000 apps.api.app.main:app
    read_only: true
    tmpfs:
      - /tmp
      - /var/tmp
    stop_grace_period: 20s


volumes:
  redis-data: {}
