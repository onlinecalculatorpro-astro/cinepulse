# -----------------------------------------------------------------------------
# Core runtime
# -----------------------------------------------------------------------------

ENV=prod
TZ=Asia/Kolkata
LOG_LEVEL=INFO
LANG=C.UTF-8
PYTHONUNBUFFERED=1

# Used by workers.jobs to categorize stories if nothing else matches.
# Safe default is "entertainment".
FALLBACK_VERTICAL=entertainment


# -----------------------------------------------------------------------------
# Storage / queues
# -----------------------------------------------------------------------------

# Postgres is currently not used in the main ingest pipeline.
# It's here for future analytics / persistence.
DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/cinepulse

# Redis is required. All services talk to the same Redis.
REDIS_URL=redis://redis:6379/0

# rq workers also look at RQ_REDIS_URL. We keep it explicit for manual runs.
RQ_REDIS_URL=redis://redis:6379/0


# -----------------------------------------------------------------------------
# Feed storage / dedupe (sanitizer + api)
# -----------------------------------------------------------------------------

# Redis LIST that holds the final public feed. Newest items live at index 0.
# Only the sanitizer worker is allowed to write to this.
FEED_KEY=feed:items

# Redis SET of canonical “already published” signatures.
# Prevents us from posting duplicate stories about the same event.
SEEN_KEY=feed:seen_signatures

# Max length of FEED_KEY. After sanitizer inserts a new story,
# it trims the list to this many items. Set <=0 to disable trimming.
MAX_FEED_LEN=200

# Realtime fanout channels:
# - FEED_PUBSUB: sanitizer publishes here when a new story is accepted;
#                api/realtime websocket streams it to clients.
# - FEED_STREAM: sanitizer XADDs tiny events here;
#                api/realtime SSE endpoint streams it.
FEED_PUBSUB=feed:pub
FEED_STREAM=feed:stream
FEED_STREAM_MAXLEN=5000

# How often (in seconds) the SSE endpoint sends keep-alive "heartbeat"
# comments so proxies/load balancers don't kill the connection.
SSE_HEARTBEAT_SEC=20


# -----------------------------------------------------------------------------
# Push notifications
# -----------------------------------------------------------------------------

# High-level switch in sanitizer:
# If this is "1"/true, sanitizer will enqueue push jobs to the "push" queue
# whenever a new story is accepted.
ENABLE_PUSH_NOTIFICATIONS=0

# High-level switch in the dedicated push worker:
# If this is "0", the push worker just no-ops even if jobs are enqueued.
PUSH_ENABLED=0

# Redis keys for push subscriber state (shared with apps/api/app/push.py)
PUSH_SET=push:tokens
PUSH_META=push:meta
PUSH_TOPIC_PREFIX=push:topic:
PUSH_DEFAULT_TOPIC=all

# Safety valves:
# - PUSH_MAX_TOKENS_PER_STORY limits blast radius for any one story
# - PUSH_BODY_MAX_CHARS limits preview body length in each notification
PUSH_MAX_TOKENS_PER_STORY=5000
PUSH_BODY_MAX_CHARS=180

# Future: if/when you wire FCM/APNs, the push worker can read credentials here.
# Right now apps/workers/push.py just prints instead of calling FCM.
FCM_SERVER_KEY=


# -----------------------------------------------------------------------------
# API service (FastAPI + gunicorn layer)
# -----------------------------------------------------------------------------

# Public base URL your frontend uses to call the API.
# api.main + img_proxy use this to generate proxied image URLs like
#   /v1/img?u=<real_image>
# so the frontend can load images without CORS drama.
API_PUBLIC_BASE_URL=https://cinepulse.netlify.app

# CORS allowlist for browsers. Comma-separated, no spaces.
CORS_ORIGINS=https://cinepulse.netlify.app,http://localhost:5173

# Optional: internal service-to-service URL that other containers
# could use to call the API directly (not required by core flow).
API_INTERNAL_BASE_URL=http://api:8000

# Optional: if you stand up the renderer service that makes share cards.
RENDERER_URL=http://renderer:8000

# Base deep link in your front-end app for a story detail view.
# API could eventually include this so clients can navigate or share.
DEEP_LINK_BASE=https://cinepulse.netlify.app/#/s


# -----------------------------------------------------------------------------
# API request limits / pagination
# -----------------------------------------------------------------------------

# When api scans Redis for /v1/feed and /v1/search it pulls in batches of this size.
BATCH_SIZE=200

# Per-IP per-minute rate limits enforced in api.main
RL_FEED_PER_MIN=120
RL_SEARCH_PER_MIN=90
RL_STORY_PER_MIN=240


# -----------------------------------------------------------------------------
# Summarizer controls (apps/workers/summarizer.py)
# -----------------------------------------------------------------------------

# Workers generate story.summary using these knobs.

# Target word count for the summary.
SUMMARY_TARGET_WORDS=85

# Hard bounds for acceptable summary size.
SUMMARY_MIN_WORDS=60
SUMMARY_MAX_WORDS=110

# If the cleaned source text is already short, we passthrough it
# instead of doing multi-sentence extraction. These are passthrough limits:
SUMMARY_PASSTHROUGH_MAX_WORDS=120
SUMMARY_PASSTHROUGH_MAX_CHARS=900


# -----------------------------------------------------------------------------
# Observability / error reporting
# -----------------------------------------------------------------------------

# If you wire up Sentry, api/scheduler/workers/etc can report exceptions.
SENTRY_DSN_BACKEND=


# -----------------------------------------------------------------------------
# External APIs / keys
# -----------------------------------------------------------------------------

# Your TMDB key if you later add TMDB ingestion/metadata enrichment.
TMDB_API_KEY=

# Additional external keys you may want to use downstream.
YOUTUBE_API_KEY=
OMDB_API_KEY=

# NOTE:
# ENABLE_TMDB_INGEST exists in some envs as a future toggle. We haven't wired
# TMDB polling into scheduler.main yet. You can leave it commented or set it.
ENABLE_TMDB_INGEST=true


# -----------------------------------------------------------------------------
# Scheduler / ingestion loop (apps/scheduler/main.py)
# -----------------------------------------------------------------------------

# scheduler reads source definitions (YouTube channels, RSS feeds, per-domain throttle)
# from a YAML file. This is mounted into the container at runtime.
SOURCES_FILE=/app/infra/source.yml

# Tell scheduler/webhooks to actually consult that YAML.
# If false, we fall back to the YT_CHANNELS / RSS_FEEDS env lists below.
USE_SOURCES_FILE=true

# Fallback YouTube channels if the YAML is missing or disabled.
# Comma-separated channel IDs.
YT_CHANNELS=UCq-Fj5jknLsUf-MWSy4_brA,UCI5q9xeOp2-1bV7xEtKgzA,UCQ7oCq4qjTLnkaW3r4U1H0g,UCwo9ey4fQ8h0c8rQ2n2-2sw

# Fallback RSS feeds if YAML is missing or disabled.
# Format: URL|kind_hint
RSS_FEEDS=https://www.bollywoodhungama.com/rss/news.xml|news,https://www.filmcompanion.in/feed|news,https://www.pinkvilla.com/entertainment/feed|news,https://www.koimoi.com/feed/|news,https://www.tellyupdates.com/feed/|news

# How often the scheduler should start a full poll cycle (minutes).
# REQUIRED unless provided in source.yml -> scheduler.poll_interval_min
POLL_INTERVAL_MIN=5

# How "fresh" content must be to even consider polling/publishing (hours).
# REQUIRED unless provided in source.yml -> scheduler.published_after_hours
PUBLISHED_AFTER_HOURS=1

# Delay (seconds) between polling each individual source *within* a cycle.
# Helps us not hammer multiple sources back-to-back.
POLL_SPREAD_SEC=2.0

# Extra random jitter (seconds) added before the next cycle sleep.
# This prevents us from hitting feeds on the same exact wall-clock schedule forever.
POLL_JITTER_SEC=10

# Hard max items to pull from any single YT channel / RSS feed per cycle,
# unless that source has its own override in the YAML.
YT_MAX_ITEMS=50
RSS_MAX_ITEMS=200

# Run only one cycle and then exit (useful for CI or manual runs). "true"/"1" to enable.
ONE_SHOT=false

# Feature flags for ingestion. If you turn off RSS or YouTube here,
# scheduler just won't enqueue those.
ENABLE_YOUTUBE_INGEST=true
ENABLE_RSS_INGEST=true


# -----------------------------------------------------------------------------
# Webhooks service (apps/webhooks/main.py)
# -----------------------------------------------------------------------------

# The public base URL where the webhooks service is reachable.
# Needed so we can tell YouTube's PubSubHubbub hub:
#   "send notifications to https://<PUBLIC_BASE_URL>/websub/yt/<channel_id>"
PUBLIC_BASE_URL=https://hooks.example.com

# When we subscribe to YouTube's hub, we ask for this lease length (seconds)
# before it expires and we must resubscribe.
WEBHOOK_LEASE_SEC=86400

# Timeout (seconds) for outbound HTTP calls from the webhook service
# to hubs (subscribe/unsubscribe).
PUSH_HTTP_TIMEOUT=8.0

# If true, on startup the webhooks service will read SOURCES_FILE,
# find enabled YouTube channels, and auto-subscribe them at the hub.
AUTO_SUBSCRIBE_ON_START=false

# How far back (hours) we look when a YouTube notification hits
# and we do an immediate "youtube_rss_poll(channel_id, published_after=since)".
# If not provided, webhooks will try to read an equivalent from SOURCES_FILE
# (youtube.defaults.published_after_hours) or fall back to ~72h.
YT_PULL_WINDOW_HOURS=72

# For generic RSS WebSub callbacks, how far back we consider items
# when we trigger rss_poll() as a reaction to a push.
RSS_PULL_WINDOW_HOURS=48

# Optional shared secret for signed WebSub notifications.
# If set, we'll verify X-Hub-Signature / X-Hub-Signature-256 HMAC.
WEBHOOK_SHARED_SECRET=
